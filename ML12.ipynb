{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a1cd48-ae8d-42ac-8500-ebbd28ea7291",
   "metadata": {},
   "source": [
    "1.What is prior probability? Give an example.\n",
    "Ans:-\n",
    "\n",
    "    Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default rate on     a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an outcome given the           evidence, that is, when the value of X is known.\n",
    "    \n",
    "    Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns,       urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the  m     probability that urn A is chosen is 0.5.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "2.What is posterior probability? Give an example.\n",
    "Ans:-\n",
    "\n",
    "     Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns,        urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the        probability that urn A is chosen is 0.5.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "3.What is likelihood probability? Give an example.\n",
    "Ans:-\n",
    "\n",
    "     Now suppose the same coin is tossed 50 times, and it shows heads only 14 times. You would assume that the likelihood of the unbiased        coin is very low. If the coin were fair, it would have shown heads and tails the same number of times.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "4.What is Naïve Bayes classifier? Why is it named so?\n",
    "Ans:-\n",
    "\n",
    "     Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for      real data; however, the technique is very effective on a large range of complex problems.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "5.What is optimal Bayes classifier?\n",
    "Ans:-\n",
    "\n",
    "     Bayes Optimal Classifier is a probabilistic model that finds the most probable prediction using the training data and space of              hypotheses to make a prediction for a new data instance.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "6.Write any two features of Bayesian learning methods.\n",
    "Ans:-\n",
    "\n",
    "     Bayesian methods aid in understanding other learning algorithms. Training examples have an incremental effect on estimated                  probabilities of hypothesis correctness. Prior knowledge and observed data combined to determine probabilities of hypotheses.                Hypotheses can make probabilistic predictions\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "7 Define the concept of consistent learners.\n",
    "Ans:-\n",
    "  \n",
    "     Consistent Learners. • A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a        hypothesis with zero error on D whenever H contains such a hypothesis. • By definition, a consistent learner must produce a hypothesis      in the version space for H given D.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "8.Write any two strengths of Bayes classifier.\n",
    "Ans:-\n",
    "\n",
    "      It is simple and easy to implement. It doesn't require as much training data. It handles both continuous and discrete data.\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "9.Write any two weaknesses of Bayes classifier.\n",
    "Ans:-\n",
    "      \n",
    "      Conditional Independence Assumption does not always hold. ...\n",
    "\n",
    "      Zero probability problem : When we encounter words in the test data for a particular class that are not present in the training data,       we might end up with zero class probabilities.         \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "10.Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1.Text classification\n",
    "\n",
    "2.Spam filtering\n",
    "\n",
    "3.Market sentiment analysis\n",
    "\n",
    "Ans:-\n",
    "\n",
    "     Naive Bayes is a learning algorithm commonly applied to text classification. Some of the applications of the Naive Bayes classifier          are: (Automatic) Classification of emails in folders, so incoming email messages go into folders such as: “Family”, “Friends”,              “Updates”,  \"Promotions”, etc.\n",
    "     \n",
    "     Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-        mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.\n",
    "     \n",
    "     The Bayes theorem is used by the Naive Bayes Classifier to forecast membership probabilities for each class, such as the likelihood          that a given record or data point belongs to that class. The most likely class is defined as the one having the highest probability\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
