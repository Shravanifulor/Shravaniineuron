{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8f1d19-6314-46e6-9c81-5d1c58807ad8",
   "metadata": {},
   "source": [
    "1.What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "Ans:-\n",
    "\n",
    "      Feature engineering refers to manipulation — addition, deletion, combination, mutation — of your data set to improve machine learning       model training, leading to better performance and greater accuracy. Effective feature engineering is based on sound knowledge of the         business problem and the available data sources.\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "2.What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    "Ans:-\n",
    "  \n",
    "     Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in        data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you        are trying to solve.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "3.Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "Ans:-\n",
    "  \n",
    "     Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the              usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared to wrapper methods as          they do not involve training the models.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction               algorithms?     \n",
    "Ans:-\n",
    "\n",
    "      Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in         data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you         are trying to solve.\n",
    "      \n",
    "       Feature extraction refers to the process of transforming raw data into numerical features that can be processed while preserving the        information in the original data set. It yields better results than applying machine learning directly to the raw data.\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "5.Describe the feature engineering process in the sense of a text categorization issue.\n",
    "Ans:-\n",
    "\n",
    "      Feature engineering is one of the most important steps in machine learning. It is the process of using domain knowledge of the data to       create features that make machine learning algorithms work. It helps us to create better data which helps the model to understand it         well and provide reasonable results.\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "6.What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3,    3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "Ans:-\n",
    "\n",
    "      Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle           between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure               document similarity in text analysis.\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "   calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "    0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "Ans:-\n",
    "\n",
    "      While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are               different. Hamming distance can also be calculated by taking XOR of two code words.\n",
    "      \n",
    "      Jaccard Coefficient Index is defined as the ratio of total elements of intersection and union of two sets. For two disjoint sets, the       value of the Jaccard index is zero. The members of two set more common relatively when the Jaccard Index is Closer to 1.\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "8.State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine     learning techniques on a data set with many dimensions? What can be done about it?\n",
    "Ans:-\n",
    "\n",
    "     High-dimensional data are defined as data in which the number of features (variables observed), p, are close to or larger than the          number of observations (or data points), n. The opposite is low-dimensional data in which the number of observations, n, far outnumbers      the number of features, p.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "9.Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2.Use of vectors\n",
    "\n",
    "3.Embedded technique\n",
    "Ans:-\n",
    "\n",
    "     Principal component analysis, or PCA, is a statistical procedure that allows you to summarize the information content in large data          tables by means of a smaller set of “summary indices” that can be more easily visualized and analyzed.\n",
    "     \n",
    "     Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of              magnitude and direction and are drawn as arrows. The length represents the magnitude and the direction of that quantity is the              direction in which the vector is pointing.\n",
    "     \n",
    "     Embedded methods combine the qualities' of filter and wrapper methods. It's implemented by algorithms that have their own built-in          feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt              penalization functions to reduce overfitting.\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "10.Make a comparison between:\n",
    "\n",
    "1.Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2.Function selection methods: filter vs. wrapper\n",
    "\n",
    "3.SMC vs. Jaccard coefficient\n",
    "Ans:-\n",
    "\n",
    "     ackward elimination starts with all features included in the model and then removes the least relevant features one at a time. Forward      selection starts with no features included in the model and then adds the most relevant features one at a time.\n",
    "     \n",
    "     The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features      by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a      model on it.\n",
    "     \n",
    "     Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent      in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts            mutual presence as matches and compares it to the ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
